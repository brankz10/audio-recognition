{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adequate-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precise-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "from src.dataset import AudioToTextDataLayer\n",
    "from src.helpers import process_evaluation_batch, process_evaluation_epoch, add_ctc_labels, AmpOptimizations, print_dict, __ctc_decoder_predictions_tensor\n",
    "from src.model import AudioPreprocessing, GreedyCTCDecoder, JasperEncoderDecoder\n",
    "from src.parts.features import audio_from_file\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cellular-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(audio_processor, encoderdecoder, greedy_decoder, audio, audio_len, labels):\n",
    "            features = audio_processor(audio, audio_len)\n",
    "            torch.cuda.synchronize()\n",
    "            t0 = time.perf_counter()\n",
    "            t_log_probs_e = encoderdecoder(features[0])\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.perf_counter()\n",
    "            t_predictions_e = greedy_decoder(log_probs=t_log_probs_e)\n",
    "            hypotheses = __ctc_decoder_predictions_tensor(t_predictions_e, labels=labels)\n",
    "            print(\"INFERENCE TIME\\t\\t: {} ms\".format((t1-t0)*1000.0))\n",
    "            print(\"TRANSCRIPT\\t\\t:\", hypotheses[0])\n",
    "\n",
    "\n",
    "def inference(wav, model, model_toml, seed=42, cudnn_benchmark=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = cudnn_benchmark\n",
    "    print(\"CUDNN BENCHMARK \", cudnn_benchmark)\n",
    "\n",
    "    optim_level = 0\n",
    "\n",
    "    jasper_model_definition = toml.load(model_toml)\n",
    "    dataset_vocab = jasper_model_definition['labels']['labels']\n",
    "    ctc_vocab = add_ctc_labels(dataset_vocab)\n",
    "\n",
    "    featurizer_config = jasper_model_definition['input_eval']\n",
    "    featurizer_config[\"optimization_level\"] = optim_level\n",
    "    use_conv_mask = jasper_model_definition['encoder'].get('convmask', True)\n",
    "        \n",
    "    print('=== model_config ===')\n",
    "    print_dict(jasper_model_definition)\n",
    "    print('=== feature_config ===')\n",
    "    print_dict(featurizer_config)\n",
    "    data_layer = None\n",
    "    \n",
    "    audio_preprocessor = AudioPreprocessing(**featurizer_config)\n",
    "    encoderdecoder = JasperEncoderDecoder(jasper_model_definition=jasper_model_definition, feat_in=1024, num_classes=len(ctc_vocab))\n",
    "\n",
    "    print(\"loading model from \", model)\n",
    "    checkpoint = torch.load(model, map_location=\"cpu\")\n",
    "    for k in audio_preprocessor.state_dict().keys():\n",
    "        checkpoint['state_dict'][k] = checkpoint['state_dict'].pop(\"audio_preprocessor.\" + k)\n",
    "    audio_preprocessor.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    encoderdecoder.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n",
    "    greedy_decoder = GreedyCTCDecoder()\n",
    "\n",
    "    print (\"audio_preprocessor.normalize: \", audio_preprocessor.featurizer.normalize)\n",
    "\n",
    "    audio_preprocessor.eval()\n",
    "    encoderdecoder.eval()\n",
    "    greedy_decoder.eval()\n",
    "    \n",
    "    audio, audio_len = audio_from_file(wav)\n",
    "    \n",
    "    run_once(audio_processor, encoderdecoder, greedy_decoder, audio, audio_len, ctc_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN BENCHMARK  False\n",
      "=== model_config ===\n",
      "Arguments:\n",
      "\t   encoder : {'activation': 'relu', 'convmask': True}\n",
      "\t     input : {'normalize': 'per_feature', 'sample_rate': 16000, 'window_size': 0.02, 'window_stride': 0.01, 'window': 'hann', 'features': 64, 'n_fft': 512, 'frame_splicing': 1, 'dither': 1e-05, 'feat_type': 'logfbank', 'normalize_transcripts': True, 'trim_silence': True, 'pad_to': 16, 'max_duration': 15, 'speed_perturbation': True, 'cutout_rect_regions': 0, 'cutout_rect_time': 60, 'cutout_rect_freq': 25, 'cutout_x_regions': 2, 'cutout_y_regions': 2, 'cutout_x_width': 6, 'cutout_y_width': 6}\n",
      "\tinput_eval : {'normalize': 'per_feature', 'sample_rate': 16000, 'window_size': 0.02, 'window_stride': 0.01, 'window': 'hann', 'features': 64, 'n_fft': 512, 'frame_splicing': 1, 'dither': 1e-05, 'feat_type': 'logfbank', 'normalize_transcripts': True, 'trim_silence': True, 'pad_to': 16, 'optimization_level': 0}\n",
      "\t    jasper : [{'filters': 256, 'repeat': 1, 'kernel': [11], 'stride': [2], 'dilation': [1], 'dropout': 0.2, 'residual': False}, {'filters': 256, 'repeat': 5, 'kernel': [11], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 256, 'repeat': 5, 'kernel': [11], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 384, 'repeat': 5, 'kernel': [13], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 384, 'repeat': 5, 'kernel': [13], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 512, 'repeat': 5, 'kernel': [17], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 512, 'repeat': 5, 'kernel': [17], 'stride': [1], 'dilation': [1], 'dropout': 0.2, 'residual': True, 'residual_dense': True}, {'filters': 640, 'repeat': 5, 'kernel': [21], 'stride': [1], 'dilation': [1], 'dropout': 0.3, 'residual': True, 'residual_dense': True}, {'filters': 640, 'repeat': 5, 'kernel': [21], 'stride': [1], 'dilation': [1], 'dropout': 0.3, 'residual': True, 'residual_dense': True}, {'filters': 768, 'repeat': 5, 'kernel': [25], 'stride': [1], 'dilation': [1], 'dropout': 0.3, 'residual': True, 'residual_dense': True}, {'filters': 768, 'repeat': 5, 'kernel': [25], 'stride': [1], 'dilation': [1], 'dropout': 0.3, 'residual': True, 'residual_dense': True}, {'filters': 896, 'repeat': 1, 'kernel': [29], 'stride': [1], 'dilation': [2], 'dropout': 0.4, 'residual': False}, {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1], 'dropout': 0.4, 'residual': False}]\n",
      "\t    labels : {'labels': [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ã', 'á', 'à', 'é', 'ê', 'í', 'ó', 'õ', 'ô', 'ú', 'ç', '<BLANK>']}\n",
      "\t     model : Jasper\n",
      "=== feature_config ===\n",
      "Arguments:\n",
      "\t               dither : 1e-05\n",
      "\t            feat_type : logfbank\n",
      "\t             features : 64\n",
      "\t       frame_splicing : 1\n",
      "\t                n_fft : 512\n",
      "\t            normalize : per_feature\n",
      "\tnormalize_transcripts : True\n",
      "\t   optimization_level : 0\n",
      "\t               pad_to : 16\n",
      "\t          sample_rate : 16000\n",
      "\t         trim_silence : True\n",
      "\t               window : hann\n",
      "\t          window_size : 0.02\n",
      "\t        window_stride : 0.01\n",
      "loading model from  ../models/Jasper_1612265229.5877585-epoch-179.pt\n"
     ]
    }
   ],
   "source": [
    "inference(\"example1.wav\", \"../models/Jasper_1612265229.5877585-epoch-179.pt\", \"../configs/jasper10x5dr_sp_offline_specaugment.toml\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "further-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "CHUNK = 4024\n",
    "WIDTH = 2\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 5\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=p.get_format_from_width(WIDTH),\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                output=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "frames = []\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    frames.append(stream.read(CHUNK))  #read audio stream\n",
    "#     stream.write(data, CHUNK)  #play back audio stream\n",
    "\n",
    "data = b''.join(frames)\n",
    "print(\"* done\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(data)\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-scale",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
